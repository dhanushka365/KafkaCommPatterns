# KafkaCommPatterns

KafkaCommPatterns is a comprehensive .NET 8 project that demonstrates core messaging patterns using Apache Kafka. This project showcases real-world implementation of event-driven architecture with multiple producer-consumer patterns for weekly comparison job processing.

## ğŸ—ï¸ Architecture Overview

This project implements a microservices-based architecture with the following components:

- **API Layer**: ASP.NET Core Web API that serves as the entry point for job creation requests
- **Common Library**: Shared components including Kafka producers, consumers, and data models
- **Consumer Service**: Background service that processes messages from Kafka topics
- **Event-Driven Communication**: Asynchronous message processing using Apache Kafka

## ğŸ“Š Event Flow Diagram

The following diagram illustrates the create request event flow in our system:

![Create Request Event Flow](images/createflow.png)

## ğŸš€ Features

### Multiple Producer Patterns
- **Automatic Producer**: Handles automated weekly comparison job creation
- **Clustering Producer**: Manages clustering-based comparison jobs
- **Express Producer**: Processes high-priority express comparison jobs
- **Manual Producer**: Handles manually triggered comparison jobs

### Consumer Patterns
- Event-driven message consumption
- Robust error handling and retry mechanisms
- Scalable message processing architecture

### Technology Stack
- **.NET 8**: Latest version of .NET for modern application development
- **Apache Kafka**: Distributed streaming platform for event-driven architecture
- **Confluent.Kafka**: Official .NET client for Apache Kafka
- **ASP.NET Core**: Web API framework
- **Swagger/OpenAPI**: API documentation and testing

## ğŸ“ Project Structure

```
KafkaCommPatterns/
â”œâ”€â”€ API/                              # Web API project
â”‚   â”œâ”€â”€ Controllers/                  # API controllers
â”‚   â”‚   â””â”€â”€ WeeklyComparisonJobsController.cs
â”‚   â”œâ”€â”€ DTO/                         # Data transfer objects
â”‚   â”œâ”€â”€ Services/                    # Application services
â”‚   â”œâ”€â”€ appsettings.json            # Configuration files
â”‚   â””â”€â”€ Program.cs                  # Application entry point
â”œâ”€â”€ COMMON/                          # Shared library
â”‚   â”œâ”€â”€ Producers/                   # Kafka producer implementations
â”‚   â”œâ”€â”€ Consumers/                   # Kafka consumer implementations
â”‚   â”œâ”€â”€ Models/                      # Data models and DTOs
â”‚   â””â”€â”€ Configuration/               # Kafka and app configuration
â”œâ”€â”€ CONSUMER/                        # Consumer service project
â”‚   â”œâ”€â”€ Entities/                    # Domain entities
â”‚   â””â”€â”€ Program.cs                  # Consumer application entry point
â”œâ”€â”€ src/                            # Additional source components
â”‚   â”œâ”€â”€ KafkaCommPatterns.Api/      # API implementation
â”‚   â”œâ”€â”€ KafkaCommPatterns.Common/   # Common utilities
â”‚   â””â”€â”€ KafkaCommPatterns.Consumer/ # Consumer implementation
â””â”€â”€ docker/                        # Docker configuration files
```

## ğŸ› ï¸ Prerequisites

Before running this project, ensure you have the following installed:

- [.NET 8 SDK](https://dotnet.microsoft.com/download/dotnet/8.0)
- [Apache Kafka](https://kafka.apache.org/downloads) (or Docker for containerized setup)
- [Docker](https://www.docker.com/) (optional, for containerized Kafka)

## âš™ï¸ Configuration

### Kafka Configuration

Update the `appsettings.json` files in both API and Consumer projects with your Kafka broker settings:

```json
{
  "Kafka": {
    "ProducerConfig": {
      "BootstrapServers": "localhost:9092",
      "Acks": "All",
      "Retries": 3
    },
    "ConsumerConfig": {
      "BootstrapServers": "localhost:9092",
      "GroupId": "weekly-comparison-group",
      "AutoOffsetReset": "Earliest"
    },
  }
}
```
## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone <repository-url>
cd KafkaCommPatterns
```

### 2. Start Kafka (Using Docker)
```bash
# Start Kafka and Zookeeper using Docker Compose
docker-compose up -d
```

### 3. Build the Solution
```bash
# Build the entire solution
dotnet build API.sln
```

### 4. Run the Services

#### Start the API Service
```bash
cd API
dotnet run
```
The API will be available at `https://localhost:7001` (or configured port)

#### Start the Consumer Service
```bash
cd CONSUMER
dotnet run
```

### 5. Test the API
Navigate to `https://localhost:7001/swagger` to access the Swagger UI and test the endpoints.

## ğŸ”Œ API Endpoints


## ğŸ“¨ Message Flow

### Producer Workflow
1. API receives HTTP request for job creation
2. Request is validated and transformed into appropriate message format
3. Message is published to corresponding Kafka topic based on job type
4. Producer confirms message delivery
5. API returns job ID and status to client

### Consumer Workflow
1. Consumer subscribes to relevant Kafka topics
2. Messages are consumed and deserialized
3. Business logic processes the job request
4. Job status is updated in the system
5. Notifications or further events may be triggered

## ğŸƒâ€â™‚ï¸ Development

### Running Tests
```bash
# Run all tests
dotnet test

# Run tests with coverage
dotnet test --collect:"XPlat Code Coverage"
```

### Code Style
This project follows standard .NET coding conventions. Consider using:
- EditorConfig for consistent formatting
- StyleCop for code analysis
- SonarQube for code quality metrics

## ğŸ³ Docker Support

### Building Docker Images
```bash
# Build API image
docker build -f API/Dockerfile -t kafkacommpatterns-api .

# Build Consumer image  
docker build -f CONSUMER/Dockerfile -t kafkacommpatterns-consumer .
```

### Docker Compose
```bash
# Start entire stack including Kafka
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

## ğŸ“ Monitoring and Logging

- **Structured Logging**: Uses built-in .NET logging with structured output
- **Health Checks**: API includes health check endpoints
- **Metrics**: Consider integrating with Prometheus/Grafana for monitoring
- **Kafka Monitoring**: Use Kafka Manager or Confluent Control Center

## ğŸ”§ Troubleshooting

### Common Issues

1. **Kafka Connection Issues**
   - Verify Kafka is running: `docker ps` or check Kafka logs
   - Ensure correct bootstrap servers in configuration
   - Check network connectivity and firewall settings

2. **Message Serialization Errors**
   - Verify message schema compatibility
   - Check Confluent Schema Registry configuration
   - Validate JSON serialization settings

3. **Consumer Lag Issues**
   - Monitor consumer group lag: `kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group weekly-comparison-group`
   - Scale consumer instances if needed
   - Optimize message processing logic

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Resources

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Confluent .NET Client](https://docs.confluent.io/kafka-clients/dotnet/current/overview.html)
- [.NET 8 Documentation](https://docs.microsoft.com/en-us/dotnet/)
- [Event-Driven Architecture Patterns](https://microservices.io/patterns/data/event-driven-architecture.html)

## ğŸ“§ Support

For questions and support, please:
- Check the [Issues](../../issues) section
- Review the [Discussions](../../discussions) for community help
- Consult the troubleshooting section above

---

**Happy Coding!** ğŸ‰